{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d822b61c-c798-4930-b53f-f74123767ef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 22:23:46.011903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-24 22:23:46.029495: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-24 22:23:46.029527: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-24 22:23:46.040101: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-24 22:23:47.179200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"ted_tflite_models/ted_model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ae3e4c-b11e-44cc-a7e4-d99f985f63b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor found: {'name': 'ted/up_conv_block_2/sequential_2/conv2d_transpose_2/BiasAdd;ted/up_conv_block_2/sequential_2/conv2d_transpose_2/conv2d_transpose;ted/up_conv_block_2/sequential_2/conv2d_transpose_2/BiasAdd/ReadVariableOp', 'index': 53, 'shape': array([  1, 176, 176,  16], dtype=int32), 'shape_signature': array([  1, 176, 176,  16], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "# Получаем информацию о всех тензорах\n",
    "tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "# Поиск и вывод информации о tensor#58\n",
    "tensor_index = 53  # Это индекс тензора, указанный в предупреждении\n",
    "\n",
    "for tensor in tensor_details:\n",
    "    if tensor['index'] == tensor_index:\n",
    "        print(f\"Tensor found: {tensor}\")\n",
    "        break\n",
    "else:\n",
    "    print(f\"Tensor with index {tensor_index} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94820ea-460c-44ee-b717-f856723acb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 22:24:02.712953: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.042975: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.043429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.045394: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.045819: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.046190: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.104802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.105007: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.105140: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-24 22:24:03.125250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 428 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpstciv092/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpstciv092/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1724527446.749908   12233 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1724527446.749965   12233 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-08-24 22:24:06.751286: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpstciv092\n",
      "2024-08-24 22:24:06.771984: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-08-24 22:24:06.772021: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpstciv092\n",
      "2024-08-24 22:24:06.833077: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-08-24 22:24:06.839673: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-08-24 22:24:07.091293: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmpstciv092\n",
      "2024-08-24 22:24:07.139786: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 388506 microseconds.\n",
      "2024-08-24 22:24:07.713289: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-24 22:24:08.386047: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3064] Estimated count of arithmetic ops: 1.799 G  ops, equivalently 0.899 G  MACs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "246164"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "model = tf.keras.layers.TFSMLayer('best_model', call_endpoint='serving_default')\n",
    "\n",
    "fixed_input = tf.keras.Input(shape=(352, 352, 3), batch_size=1)\n",
    "\n",
    "fixed_model = tf.keras.Model(inputs=fixed_input, outputs=model(fixed_input))\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(fixed_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"ted_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tflite_model_file = tflite_models_dir/\"ted_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24d5c28-992d-4718-bcc0-a7fd35cdfb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TFLite ModelAnalyzer ===\n",
      "\n",
      "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the CONV_2D op takes\n",
      "tensor #0 and tensor #37 and tensor #38 as input and produces tensor #39 as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#46, T#65, T#57, T#55]\n",
      "  Op#0 CONV_2D(T#0, T#37, T#38) -> [T#39]\n",
      "  Op#1 CONV_2D(T#39, T#33, T#8) -> [T#40]\n",
      "  Op#2 CONV_2D(T#40, T#32, T#7) -> [T#41]\n",
      "  Op#3 CONV_2D(T#41, T#31, T#17) -> [T#42]\n",
      "  Op#4 MAX_POOL_2D(T#42) -> [T#43]\n",
      "  Op#5 CONV_2D(T#43, T#30, T#16) -> [T#44]\n",
      "  Op#6 CONV_2D(T#42, T#29, T#6) -> [T#45]\n",
      "  Op#7 TRANSPOSE_CONV(T#35[1, 352, 352, 1], T#21, T#45, T#15) -> [T#46]\n",
      "  Op#8 CONV_2D(T#40, T#28, T#14) -> [T#47]\n",
      "  Op#9 ADD(T#43, T#47) -> [T#48]\n",
      "  Op#10 CONV_2D(T#48, T#27, T#5) -> [T#49]\n",
      "  Op#11 CONV_2D(T#49, T#26, T#16) -> [T#50]\n",
      "  Op#12 ADD(T#50, T#44) -> [T#51]\n",
      "  Op#13 CONV_2D(T#51, T#1, T#4) -> [T#52]\n",
      "  Op#14 TRANSPOSE_CONV(T#34[1, 176, 176, 16], T#20, T#52, T#13) -> [T#53]\n",
      "  Op#15 CONV_2D(T#53, T#25, T#3) -> [T#54]\n",
      "  Op#16 TRANSPOSE_CONV(T#35[1, 352, 352, 1], T#19, T#54, T#12) -> [T#55]\n",
      "  Op#17 CONV_2D(T#40, T#24, T#2) -> [T#56]\n",
      "  Op#18 TRANSPOSE_CONV(T#35[1, 352, 352, 1], T#18, T#56, T#11) -> [T#57]\n",
      "  Op#19 CONCATENATION(T#57, T#46, T#55) -> [T#58]\n",
      "  Op#20 RELU(T#58) -> [T#59]\n",
      "  Op#21 DEPTHWISE_CONV_2D(T#59, T#23, T#10) -> [T#60]\n",
      "  Op#22 RELU(T#60) -> [T#61]\n",
      "  Op#23 DEPTHWISE_CONV_2D(T#61, T#22, T#9) -> [T#62]\n",
      "  Op#24 ADD(T#62, T#60) -> [T#63]\n",
      "  Op#25 SUM(T#63, T#36[-1]) -> [T#64]\n",
      "  Op#26 RESHAPE(T#64, T#35[1, 352, 352, 1]) -> [T#65]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(serving_default_inputs:0) shape:[1, 352, 352, 3], type:FLOAT32\n",
      "  T#1(arith.constant) shape:[16, 1, 1, 48], type:FLOAT32 RO 3072 bytes, buffer: 2, data:[-0.0947592, 0.0646488, 0.00919607, -0.0204261, 0.147917, ...]\n",
      "  T#2(arith.constant1) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 3, data:[-0.0138866]\n",
      "  T#3(arith.constant2) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 4, data:[-0.00525021]\n",
      "  T#4(arith.constant3) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 5, data:[0.00136966, 0.0186065, 0.0131717, 0.0160598, -0.0293352, ...]\n",
      "  T#5(arith.constant4) shape:[48], type:FLOAT32 RO 192 bytes, buffer: 6, data:[0.0101723, 0.0353445, 0.0132953, 0.0168071, 0.00951816, ...]\n",
      "  T#6(arith.constant5) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 7, data:[0.0197319]\n",
      "  T#7(arith.constant6) shape:[32], type:FLOAT32 RO 128 bytes, buffer: 8, data:[0.00640245, 0.0237409, 0.0254263, 0.00661014, 0.00252233, ...]\n",
      "  T#8(arith.constant7) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 9, data:[-0.0159807, 0.0172339, 0.013234, -0.0259944, 0.00196769, ...]\n",
      "  T#9(arith.constant8) shape:[24], type:FLOAT32 RO 96 bytes, buffer: 10, data:[-0.0587757, -0.0587757, -0.0587757, -0.0587757, -0.0587757, ...]\n",
      "  T#10(arith.constant9) shape:[24], type:FLOAT32 RO 96 bytes, buffer: 11, data:[-0.0501949, -0.0590358, -0.0636052, -0.0602643, -0.0643947, ...]\n",
      "  T#11(arith.constant10) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 12, data:[0.044477]\n",
      "  T#12(arith.constant11) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 13, data:[-0.0785143]\n",
      "  T#13(arith.constant12) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 14, data:[0.00552098, -0.00549083, 0.00553306, 0.00536548, 0.00535486, ...]\n",
      "  T#14(arith.constant13) shape:[32], type:FLOAT32 RO 128 bytes, buffer: 15, data:[-0.0140052, -0.0291452, 0.0332033, -0.0446238, 0.0383043, ...]\n",
      "  T#15(arith.constant14) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 16, data:[-0.00792478]\n",
      "  T#16(arith.constant15) shape:[48], type:FLOAT32 RO 192 bytes, buffer: 17, data:[-0.00472667, 0.00893107, -0.00703632, -0.000155951, -0.0428197, ...]\n",
      "  T#17(arith.constant16) shape:[32], type:FLOAT32 RO 128 bytes, buffer: 18, data:[0.00969168, -0.0285492, 0.0256747, -0.0151225, -0.000258914, ...]\n",
      "  T#18(arith.constant17) shape:[1, 4, 4, 1], type:FLOAT32 RO 64 bytes, buffer: 19, data:[0.0636081, 0.374603, 0.417424, -0.188488, -0.0332931, ...]\n",
      "  T#19(arith.constant18) shape:[1, 4, 4, 1], type:FLOAT32 RO 64 bytes, buffer: 20, data:[0.269961, 0.121435, 0.201615, -0.160604, -0.149347, ...]\n",
      "  T#20(arith.constant19) shape:[16, 4, 4, 16], type:FLOAT32 RO 16384 bytes, buffer: 21, data:[-0.111651, -0.0718949, -0.00339815, -0.0967268, 0.0949429, ...]\n",
      "  T#21(arith.constant20) shape:[1, 4, 4, 1], type:FLOAT32 RO 64 bytes, buffer: 22, data:[-0.131041, -0.23883, 0.120373, -0.133515, -0.321124, ...]\n",
      "  T#22(arith.constant21) shape:[1, 3, 3, 24], type:FLOAT32 RO 864 bytes, buffer: 23, data:[0.0259662, 0.0771598, -0.13807, -0.114655, -0.125538, ...]\n",
      "  T#23(arith.constant22) shape:[1, 3, 3, 24], type:FLOAT32 RO 864 bytes, buffer: 24, data:[0.0348257, 0.0177861, 0.155124, -0.296038, -0.194549, ...]\n",
      "  T#24(arith.constant23) shape:[1, 1, 1, 16], type:FLOAT32 RO 64 bytes, buffer: 25, data:[-0.10393, 0.686845, -0.411899, 0.302411, 0.538051, ...]\n",
      "  T#25(arith.constant24) shape:[1, 1, 1, 16], type:FLOAT32 RO 64 bytes, buffer: 26, data:[-0.423432, 0.495196, -0.443411, -0.367863, -0.563532, ...]\n",
      "  T#26(arith.constant25) shape:[48, 3, 3, 48], type:FLOAT32 RO 82944 bytes, buffer: 27, data:[0.0148157, 0.0286942, -0.0172896, 0.146101, -0.0340067, ...]\n",
      "  T#27(arith.constant26) shape:[48, 3, 3, 32], type:FLOAT32 RO 55296 bytes, buffer: 28, data:[0.0401185, 0.0148116, -0.0670678, -0.0868289, -0.0321346, ...]\n",
      "  T#28(arith.constant27) shape:[32, 1, 1, 16], type:FLOAT32 RO 2048 bytes, buffer: 29, data:[0.251546, 0.107345, 0.214378, -0.200844, -0.231321, ...]\n",
      "  T#29(arith.constant28) shape:[1, 1, 1, 32], type:FLOAT32 RO 128 bytes, buffer: 30, data:[0.485784, -0.451089, 0.188027, 0.268339, -0.291435, ...]\n",
      "  T#30(arith.constant29) shape:[48, 1, 1, 32], type:FLOAT32 RO 6144 bytes, buffer: 31, data:[-0.0748588, 0.0696217, 0.0501374, -0.18016, -0.179884, ...]\n",
      "  T#31(arith.constant30) shape:[32, 3, 3, 32], type:FLOAT32 RO 36864 bytes, buffer: 32, data:[0.0104901, 0.0150555, -0.000942747, 0.0768391, -0.102626, ...]\n",
      "  T#32(arith.constant31) shape:[32, 3, 3, 16], type:FLOAT32 RO 18432 bytes, buffer: 33, data:[-0.032571, 0.0461075, -0.0839977, 0.028662, -0.0509518, ...]\n",
      "  T#33(arith.constant32) shape:[16, 3, 3, 16], type:FLOAT32 RO 9216 bytes, buffer: 34, data:[0.159624, -0.023422, -0.123661, 0.0479093, 0.185024, ...]\n",
      "  T#34(arith.constant33) shape:[4], type:INT32 RO 16 bytes, buffer: 35, data:[1, 176, 176, 16]\n",
      "  T#35(arith.constant34) shape:[4], type:INT32 RO 16 bytes, buffer: 36, data:[1, 352, 352, 1]\n",
      "  T#36(arith.constant35) shape:[], type:INT32 RO 4 bytes, buffer: 37, data:[-1]\n",
      "  T#37(ted/double_conv_block/conv2d/Conv2D) shape:[16, 3, 3, 3], type:FLOAT32 RO 1728 bytes, buffer: 38, data:[0.043523, 0.173581, -0.0197561, 0.107462, -0.00127782, ...]\n",
      "  T#38(ted/double_conv_block/re_lu/Relu;ted/double_conv_block/conv2d/BiasAdd;ted/double_conv_block/conv2d/Conv2D;ted/double_conv_block/conv2d/BiasAdd/ReadVariableOp) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 39, data:[0.0164306, -0.0131562, 0.00969961, 0.0256806, 0.0850697, ...]\n",
      "  T#39(ted/double_conv_block/re_lu/Relu;ted/double_conv_block/conv2d/BiasAdd;ted/double_conv_block/conv2d/Conv2D;ted/double_conv_block/conv2d/BiasAdd/ReadVariableOp1) shape:[1, 176, 176, 16], type:FLOAT32\n",
      "  T#40(ted/double_conv_block/re_lu/Relu_1;ted/double_conv_block/conv2d_1/BiasAdd;ted/double_conv_block/conv2d_1/Conv2D;ted/double_conv_block/conv2d_1/BiasAdd/ReadVariableOp) shape:[1, 176, 176, 16], type:FLOAT32\n",
      "  T#41(ted/double_conv_block_1/re_lu_1/Relu;ted/double_conv_block_1/conv2d_2/BiasAdd;ted/double_conv_block_1/conv2d_2/Conv2D;ted/double_conv_block_1/conv2d_2/BiasAdd/ReadVariableOp) shape:[1, 176, 176, 32], type:FLOAT32\n",
      "  T#42(ted/double_conv_block_1/conv2d_3/BiasAdd;ted/double_conv_block_1/conv2d_3/Conv2D;ted/double_conv_block_1/conv2d_3/BiasAdd/ReadVariableOp) shape:[1, 176, 176, 32], type:FLOAT32\n",
      "  T#43(ted/max_pooling2d/MaxPool) shape:[1, 88, 88, 32], type:FLOAT32\n",
      "  T#44(ted/single_conv_block_1/conv2d_7/BiasAdd;ted/single_conv_block_1/conv2d_7/Conv2D;) shape:[1, 88, 88, 48], type:FLOAT32\n",
      "  T#45(ted/up_conv_block_1/sequential_1/re_lu_4/Relu;ted/up_conv_block_1/sequential_1/conv2d_9/BiasAdd;ted/up_conv_block_1/sequential_1/conv2d_9/Conv2D;ted/up_conv_block_1/sequential_1/conv2d_9/BiasAdd/ReadVariableOp) shape:[1, 176, 176, 1], type:FLOAT32\n",
      "  T#46(StatefulPartitionedCall:1) shape:[1, 352, 352, 1], type:FLOAT32\n",
      "  T#47(ted/single_conv_block/conv2d_6/BiasAdd;ted/single_conv_block/conv2d_6/Conv2D;ted/single_conv_block/conv2d_6/BiasAdd/ReadVariableOp) shape:[1, 88, 88, 32], type:FLOAT32\n",
      "  T#48(ted/add) shape:[1, 88, 88, 32], type:FLOAT32\n",
      "  T#49(ted/dense_block/dense_layer/re_lu_2/Relu;ted/dense_block/dense_layer/conv2d_4/BiasAdd;ted/dense_block/dense_layer/conv2d_4/Conv2D;ted/dense_block/dense_layer/conv2d_4/BiasAdd/ReadVariableOp) shape:[1, 88, 88, 48], type:FLOAT32\n",
      "  T#50(ted/dense_block/dense_layer/conv2d_5/BiasAdd;ted/dense_block/dense_layer/conv2d_5/Conv2D;) shape:[1, 88, 88, 48], type:FLOAT32\n",
      "  T#51(ted/dense_block/dense_layer/add) shape:[1, 88, 88, 48], type:FLOAT32\n",
      "  T#52(ted/up_conv_block_2/sequential_2/re_lu_5/Relu;ted/up_conv_block_2/sequential_2/conv2d_10/BiasAdd;ted/up_conv_block_2/sequential_2/conv2d_10/Conv2D;ted/up_conv_block_2/sequential_2/conv2d_10/BiasAdd/ReadVariableOp) shape:[1, 88, 88, 16], type:FLOAT32\n",
      "  T#53(ted/up_conv_block_2/sequential_2/conv2d_transpose_2/BiasAdd;ted/up_conv_block_2/sequential_2/conv2d_transpose_2/conv2d_transpose;ted/up_conv_block_2/sequential_2/conv2d_transpose_2/BiasAdd/ReadVariableOp) shape:[1, 176, 176, 16], type:FLOAT32\n",
      "  T#54(ted/up_conv_block_2/sequential_2/re_lu_6/Relu;ted/up_conv_block_2/sequential_2/conv2d_11/BiasAdd;ted/up_conv_block_2/sequential_2/conv2d_11/Conv2D;ted/up_conv_block_2/sequential_2/conv2d_11/BiasAdd/ReadVariableOp) shape:[1, 176, 176, 1], type:FLOAT32\n",
      "  T#55(StatefulPartitionedCall:2) shape:[1, 352, 352, 1], type:FLOAT32\n",
      "  T#56(ted/up_conv_block/sequential/re_lu_3/Relu;ted/up_conv_block/sequential/conv2d_8/BiasAdd;ted/up_conv_block/sequential/conv2d_8/Conv2D;ted/up_conv_block/sequential/conv2d_8/BiasAdd/ReadVariableOp) shape:[1, 176, 176, 1], type:FLOAT32\n",
      "  T#57(StatefulPartitionedCall:0) shape:[1, 352, 352, 1], type:FLOAT32\n",
      "  T#58(ted/concat) shape:[1, 352, 352, 3], type:FLOAT32\n",
      "  T#59(ted/double_fusion/re_lu_7/Relu) shape:[1, 352, 352, 3], type:FLOAT32\n",
      "  T#60(ted/double_fusion/depthwise_conv2d/BiasAdd;ted/double_fusion/depthwise_conv2d/depthwise;ted/double_fusion/depthwise_conv2d/BiasAdd/ReadVariableOp) shape:[1, 352, 352, 24], type:FLOAT32\n",
      "  T#61(ted/double_fusion/re_lu_7/Relu_1) shape:[1, 352, 352, 24], type:FLOAT32\n",
      "  T#62(ted/double_fusion/depthwise_conv2d_1/BiasAdd;ted/double_fusion/depthwise_conv2d_1/depthwise;ted/double_fusion/depthwise_conv2d_1/BiasAdd/ReadVariableOp) shape:[1, 352, 352, 24], type:FLOAT32\n",
      "  T#63(ted/double_fusion/add) shape:[1, 352, 352, 24], type:FLOAT32\n",
      "  T#64(ted/double_fusion/Sum) shape:[1, 352, 352], type:FLOAT32\n",
      "  T#65(StatefulPartitionedCall:3) shape:[1, 352, 352, 1], type:FLOAT32\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Your TFLite model has '1' signature_def(s).\n",
      "\n",
      "Signature#0 key: 'serving_default'\n",
      "- Subgraph: Subgraph#0\n",
      "- Inputs: \n",
      "    'inputs' : T#0\n",
      "- Outputs: \n",
      "    'output_0' : T#57\n",
      "    'output_1' : T#46\n",
      "    'output_2' : T#55\n",
      "    'output_3' : T#65\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:     246164 bytes\n",
      "    Non-data buffer size:      10484 bytes (04.26 %)\n",
      "  Total data buffer size:     235680 bytes (95.74 %)\n",
      "    (Zero value buffers):          0 bytes (00.00 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecfdc2-4d78-4b13-ad7c-f0b74c2f2adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
